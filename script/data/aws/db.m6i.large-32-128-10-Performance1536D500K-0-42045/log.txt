DB Instance Type: db.m6i.large
DB Instance Provider: aws
DB enable_seqscan: on
db-label: run1-seqon
drop_old: True
load: True
search-serial: True
search-concurrent: True
case-type: Performance1536D500K
maintenance-work-mem: 4GB
max-parallel-workers: 2
ef-search: [10, 20, 40, 80, 120, 200, 400]
ef-construction: 128
m: 32
num-concurrency: 1,5,15,20,25,30,35,40,45,50,55,60,65,70
concurrency-duration: 30
run_count: 1
Current PostgreSQL configurations:
Raw query results: ['5min', '3902320kB', 'off', '4GB', '2', '2', '2', '6GB', '8', '1951160kB', 'zstd', '4MB']
checkpoint_timeout: 5min
effective_cache_size: 3902320kB
jit: off
maintenance_work_mem: 4GB
max_parallel_maintenance_workers: 2
max_parallel_workers: 2
max_parallel_workers_per_gather: 2
max_wal_size: 6GB
max_worker_processes: 8
shared_buffers: 1951160kB
wal_compression: zstd
work_mem: 4MB
Running command: vectordbbench pgvectorhnsw --user-name postgres --password Emumba123 --host aws-rds-pg-postgres.cjegs2mait4g.us-east-1.rds.amazonaws.com --db-name ann --drop-old --load --search-serial --search-concurrent --case-type Performance1536D500K --maintenance-work-mem 4GB --max-parallel-workers 2 --ef-construction 128 --m 32 --num-concurrency 1,5,15,20,25,30,35,40,45,50,55,60,65,70 --concurrency-duration 30 --ef-search 10
2024-09-02 01:07:52,496 | INFO |Task:
TaskConfig(db=<DB.PgVector: 'PgVector'>, db_config=PgVectorConfig(db_label='2024-09-02T01:07:52.483924', version='', note='', user_name=SecretStr('**********'), password=SecretStr('**********'), host='aws-rds-pg-postgres.cjegs2mait4g.us-east-1.rds.amazonaws.com', port=5432, db_name='ann'), db_case_config=PgVectorHNSWConfig(metric_type=None, create_index_before_load=False, create_index_after_load=True, m=32, ef_construction=128, ef_search=10, index=<IndexType.ES_HNSW: 'hnsw'>, maintenance_work_mem='4GB', max_parallel_workers=2), case_config=CaseConfig(case_id=<CaseType.Performance1536D500K: 10>, custom_case=None, k=100, concurrency_search_config=ConcurrencySearchConfig(num_concurrency=[1, 5, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70], concurrency_duration=30)), stages=['drop_old', 'load', 'search_serial', 'search_concurrent'])
 (cli.py:357)
2024-09-02 01:07:52,496 | DEBUG |tasks: [TaskConfig(db=<DB.PgVector: 'PgVector'>, db_config=PgVectorConfig(db_label='2024-09-02T01:07:52.483924', version='', note='', user_name=SecretStr('**********'), password=SecretStr('**********'), host='aws-rds-pg-postgres.cjegs2mait4g.us-east-1.rds.amazonaws.com', port=5432, db_name='ann'), db_case_config=PgVectorHNSWConfig(metric_type=None, create_index_before_load=False, create_index_after_load=True, m=32, ef_construction=128, ef_search=10, index=<IndexType.ES_HNSW: 'hnsw'>, maintenance_work_mem='4GB', max_parallel_workers=2), case_config=CaseConfig(case_id=<CaseType.Performance1536D500K: 10>, custom_case=None, k=100, concurrency_search_config=ConcurrencySearchConfig(num_concurrency=[1, 5, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70], concurrency_duration=30)), stages=['drop_old', 'load', 'search_serial', 'search_concurrent'])], task_label: None, dataset source: DatasetSource.S3 (interface.py:62)
2024-09-02 01:07:52,496 | INFO |generated uuid for the tasks: 8cf1485e78ef48fe86151e390e62c065 (interface.py:66)
2024-09-02 01:07:52,558 | INFO | DB             | CaseType     Dataset               Filter | task_label (task_runner.py:340)
2024-09-02 01:07:52,558 | INFO | -----------    | ------------ -------------------- ------- | -------    (task_runner.py:340)
2024-09-02 01:07:52,558 | INFO | PgVector-2024-09-02T01:07:52.483924 | Performance  OpenAI-MEDIUM-500K      None | 8cf1485e78ef48fe86151e390e62c065 (task_runner.py:340)
2024-09-02 01:07:52,558 | INFO |task submitted: id=8cf1485e78ef48fe86151e390e62c065, 8cf1485e78ef48fe86151e390e62c065, case number: 1 (interface.py:231)
2024-09-02 01:07:52,975 | INFO |[1/1] start case: {'label': <CaseLabel.Performance: 2>, 'dataset': {'data': {'name': 'OpenAI', 'size': 500000, 'dim': 1536, 'metric_type': <MetricType.COSINE: 'COSINE'>}}, 'db': 'PgVector-2024-09-02T01:07:52.483924'}, drop_old=True (interface.py:164)
2024-09-02 01:07:52,975 | INFO |Starting run (task_runner.py:100)
2024-09-02 01:07:53,052 | INFO |PgVector config values: {'host': 'aws-rds-pg-postgres.cjegs2mait4g.us-east-1.rds.amazonaws.com', 'port': 5432, 'dbname': 'ann', 'user': 'postgres', 'password': 'Emumba123'}
metric_type=<MetricType.COSINE: 'COSINE'> create_index_before_load=False create_index_after_load=True m=32 ef_construction=128 ef_search=10 index=<IndexType.ES_HNSW: 'hnsw'> maintenance_work_mem='4GB' max_parallel_workers=2 (pgvector.py:54)
2024-09-02 01:07:53,052 | INFO |PgVector client drop index : pgvector_index (pgvector.py:196)
2024-09-02 01:07:53,052 | DEBUG |DROP INDEX IF EXISTS "pgvector_index" (pgvector.py:201)
2024-09-02 01:07:53,053 | INFO |PgVector client drop table : pg_vector_collection (pgvector.py:172)
2024-09-02 01:07:53,124 | INFO |PgVector client create table : pg_vector_collection (pgvector.py:317)
2024-09-02 01:07:53,751 | INFO |Read the entire file into memory: test.parquet (dataset.py:229)
2024-09-02 01:07:53,798 | INFO |Read the entire file into memory: neighbors.parquet (dataset.py:229)
2024-09-02 01:07:53,823 | DEBUG |OpenAI: available train files ['shuffle_train.parquet'] (dataset.py:223)
2024-09-02 01:07:53,823 | INFO |Start performance case (task_runner.py:158)
2024-09-02 01:07:54,367 | DEBUG |SET "hnsw.ef_search" = "10"; (pgvector.py:137)
2024-09-02 01:07:54,368 | INFO |(SpawnProcess-1:1) Start inserting embeddings in batch 5000 (serial_runner.py:35)
2024-09-02 01:07:54,368 | INFO |Get iterator for shuffle_train.parquet (dataset.py:247)
2024-09-02 01:07:56,644 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:07:57,598 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:07:58,570 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:07:59,500 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:00,398 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:01,322 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:02,189 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:03,050 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:04,049 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:04,858 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:05,819 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:06,679 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:07,561 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:08,482 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:09,311 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:10,410 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:11,226 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:12,407 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:13,214 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:14,336 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:14,766 | INFO |(SpawnProcess-1:1) Loaded 100000 embeddings into VectorDB (serial_runner.py:59)
2024-09-02 01:08:15,204 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:16,087 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:17,006 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:17,921 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:18,810 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:19,689 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:20,527 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:21,506 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:22,310 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:23,195 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:24,010 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:24,899 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:25,759 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:26,546 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:27,731 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:28,572 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:29,821 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:31,125 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:32,604 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:33,968 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:34,492 | INFO |(SpawnProcess-1:1) Loaded 200000 embeddings into VectorDB (serial_runner.py:59)
2024-09-02 01:08:34,868 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:35,782 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:36,705 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:37,589 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:38,449 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:39,357 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:40,325 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:41,131 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:41,994 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:42,821 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:43,679 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:44,564 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:45,328 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:46,446 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:47,273 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:48,448 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:49,257 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:50,231 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:51,108 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:51,944 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:52,363 | INFO |(SpawnProcess-1:1) Loaded 300000 embeddings into VectorDB (serial_runner.py:59)
2024-09-02 01:08:52,790 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:53,642 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:54,462 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:55,350 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:56,166 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:57,097 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:57,922 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:58,838 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:08:59,587 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:00,516 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:01,435 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:02,301 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:03,832 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:05,197 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:06,942 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:07,872 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:08,974 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:09,845 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:10,773 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:11,667 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:12,259 | INFO |(SpawnProcess-1:1) Loaded 400000 embeddings into VectorDB (serial_runner.py:59)
2024-09-02 01:09:12,631 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:13,525 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:14,395 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:15,238 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:16,153 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:16,992 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:17,934 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:18,779 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:19,690 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:20,730 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:21,795 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:22,973 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:23,811 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:25,007 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:25,786 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:26,786 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:27,721 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:28,574 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:29,450 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:30,332 | DEBUG |batch dataset size: 5000, 5000 (serial_runner.py:47)
2024-09-02 01:09:30,772 | INFO |(SpawnProcess-1:1) Loaded 500000 embeddings into VectorDB (serial_runner.py:59)
2024-09-02 01:09:30,882 | INFO |(SpawnProcess-1:1) Finish loading all dataset into VectorDB, dur=96.51386132399784 (serial_runner.py:61)
2024-09-02 01:09:31,580 | DEBUG |SET "hnsw.ef_search" = "10"; (pgvector.py:137)
2024-09-02 01:09:31,581 | INFO |PgVector post insert before optimize (pgvector.py:188)
2024-09-02 01:09:31,581 | INFO |PgVector client drop index : pgvector_index (pgvector.py:196)
2024-09-02 01:09:31,581 | DEBUG |DROP INDEX IF EXISTS "pgvector_index" (pgvector.py:201)
2024-09-02 01:09:31,581 | INFO |PgVector client create index : pgvector_index (pgvector.py:276)
2024-09-02 01:09:31,585 | INFO |PgVector parallel index creation parameters: [('2',), ('2',), ('4GB',)] (pgvector.py:271)
2024-09-02 01:09:31,585 | DEBUG |
            CREATE INDEX IF NOT EXISTS  "pgvector_index"  ON public. "pg_vector_collection"  
            USING  "hnsw"  (embedding  "vector_cosine_ops" )
             WITH ( "m" = "32", "ef_construction" = "128" ); (pgvector.py:308)
2024-09-02 01:24:31,035 | WARNING |VectorDB optimize timeout in 900 (task_runner.py:251)
2024-09-02 01:24:31,046 | WARNING |Failed to run performance case, reason = Performance case optimize timeout (task_runner.py:193)
Traceback (most recent call last):
  File "/home/ubuntu/vdbbenchmark/VectorDBBench/vectordb_bench/backend/task_runner.py", line 249, in _optimize
    return future.result(timeout=self.ca.optimize_timeout)[1]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/concurrent/futures/_base.py", line 458, in result
    raise TimeoutError()
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/vdbbenchmark/VectorDBBench/vectordb_bench/backend/task_runner.py", line 165, in _run_perf_case
    m.build_dur = self._optimize()
                  ^^^^^^^^^^^^^^^^
  File "/home/ubuntu/vdbbenchmark/VectorDBBench/vectordb_bench/backend/task_runner.py", line 254, in _optimize
    raise PerformanceTimeoutError("Performance case optimize timeout") from e
vectordb_bench.models.PerformanceTimeoutError: Performance case optimize timeout
2024-09-02 01:24:31,046 | WARNING |[1/1] case {'label': <CaseLabel.Performance: 2>, 'dataset': {'data': {'name': 'OpenAI', 'size': 500000, 'dim': 1536, 'metric_type': <MetricType.COSINE: 'COSINE'>}}, 'db': 'PgVector-2024-09-02T01:07:52.483924'} failed to run, reason=Performance case optimize timeout (interface.py:179)
2024-09-02 01:24:31,047 | INFO |Task summary: run_id=8cf14, task_label=8cf1485e78ef48fe86151e390e62c065 (models.py:346)
2024-09-02 01:24:31,047 | INFO |DB       | db_label                      case                 label                            | load_dur    qps        latency(p99)    recall        max_load_count | label (models.py:346)
2024-09-02 01:24:31,047 | INFO |-------- | ----------------------------- -------------------- -------------------------------- | ----------- ---------- --------------- ------------- -------------- | ----- (models.py:346)
2024-09-02 01:24:31,047 | INFO |PgVector | 2024-09-02T01:07:52.483924    Performance1536D500K 8cf1485e78ef48fe86151e390e62c065 | 0.0         0.0        0.0             0.0           0              | ?     (models.py:346)
2024-09-02 01:24:31,047 | INFO |write results to disk results/pgvector/hnsw/run1-seqon/aws/db.m6i.large-32-128-10-Performance1536D500K-0-42045/result_20240902_8cf1485e78ef48fe86151e390e62c065_pgvector.json (models.py:204)
2024-09-02 01:24:31,047 | INFO |Success to finish task: label=8cf1485e78ef48fe86151e390e62c065, run_id=8cf1485e78ef48fe86151e390e62c065 (interface.py:203)
